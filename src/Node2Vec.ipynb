{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.image as mpimg\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/articles.csv\")\n",
    "# customers = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/customers.csv\")\n",
    "transactions = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_freq = transactions.groupby('article_id')['customer_id'].nunique()\n",
    "user_freq = transactions.groupby('customer_id')['article_id'].nunique()\n",
    "\n",
    "items = item_freq[item_freq >= 100].index\n",
    "users = user_freq[user_freq >= 100].index\n",
    "\n",
    "filtered_df = transactions[transactions['article_id'].isin(items) & transactions['customer_id'].isin(users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = filtered_df.groupby(['customer_id', 'article_id']).size().reset_index(name='frequency')\n",
    "\n",
    "GraphTravel_HM = filtered_df.merge(freq, on=['customer_id', 'article_id'], how='left')\n",
    "\n",
    "GraphTravel_HM = GraphTravel_HM[GraphTravel_HM['frequency'] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(GraphTravel_HM)\n",
    "\n",
    "print(\"unique customer_id\" , GraphTravel_HM.customer_id.nunique())\n",
    "print(\"unique article_id\" , GraphTravel_HM.article_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(GraphTravel_HM['frequency'], kde=True, bins=30)\n",
    "\n",
    "plt.title('Distribution of frequency')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_customer_ids = GraphTravel_HM['customer_id'].unique()\n",
    "customer_id_mapping = {id: i for i, id in enumerate(unique_customer_ids)}\n",
    "GraphTravel_HM['customer_id'] = GraphTravel_HM['customer_id'].map(customer_id_mapping)\n",
    "\n",
    "item_name_mapping = dict(zip(articles['article_id'], articles['prod_name'])) # prod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for index, row in GraphTravel_HM.iterrows():\n",
    "    G.add_node(row['customer_id'], type='user')\n",
    "    G.add_node(row['article_id'], type='item')\n",
    "    G.add_edge(row['customer_id'], row['article_id'], weight=row['frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biased random walk  \n",
    "def biased_random_walk(G, start_node, walk_length, p=1, q=1):\n",
    "    walk = [start_node]\n",
    "\n",
    "    while len(walk) < walk_length:\n",
    "        cur_node = walk[-1]\n",
    "        cur_neighbors = list(G.neighbors(cur_node))\n",
    "\n",
    "        if len(cur_neighbors) > 0:\n",
    "            if len(walk) == 1:\n",
    "                walk.append(random.choice(cur_neighbors))\n",
    "            else:\n",
    "                prev_node = walk[-2]\n",
    "\n",
    "                probability = []\n",
    "                for neighbor in cur_neighbors:\n",
    "                    if neighbor == prev_node:\n",
    "                        # Return parameter \n",
    "                        probability.append(1/p)\n",
    "                    elif G.has_edge(neighbor, prev_node):\n",
    "                        # Stay parameter \n",
    "                        probability.append(1)\n",
    "                    else:\n",
    "                        # In-out parameter \n",
    "                        probability.append(1/q)\n",
    "\n",
    "                probability = np.array(probability)\n",
    "                probability = probability / probability.sum()  # normalize\n",
    "\n",
    "                next_node = np.random.choice(cur_neighbors, p=probability)\n",
    "                walk.append(next_node)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_walks(G, num_walks, walk_length, p=1, q=1):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(nodes)  # to ensure randomness\n",
    "        for node in nodes:\n",
    "            walk_from_node = biased_random_walk(G, node, walk_length, p, q)\n",
    "            walks.append(walk_from_node)\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_walks(G, 2, 8, p=0.5, q=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Walk \n",
    "walks = generate_walks(G, num_walks=10, walk_length=20, p=9, q=1)\n",
    "filtered_walks = [walk for walk in walks if len(walk) >= 5]\n",
    "\n",
    "# to String  (for Word2Vec input)\n",
    "walks = [[str(node) for node in walk] for walk in walks]\n",
    "\n",
    "# Word2Vec train\n",
    "model = Word2Vec(walks, vector_size=128, window=5, min_count=0,  hs=1, sg=1, workers=4, epochs=10)\n",
    "\n",
    "# node embedding extract\n",
    "embeddings = {node_id: model.wv[node_id] for node_id in model.wv.index_to_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(user_id, embeddings):\n",
    "    return embeddings[str(user_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rated_items(user_id, df):\n",
    "    return set(df[df['customer_id'] == user_id]['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(user_id, df, embeddings):\n",
    "    rated_items = get_rated_items(user_id, df)\n",
    "    user_embedding = get_user_embedding(user_id, embeddings)\n",
    "\n",
    "    item_similarities = []\n",
    "    for item_id in set(df['article_id']):\n",
    "        if item_id not in rated_items:  \n",
    "            item_embedding = embeddings[str(item_id)]\n",
    "            similarity = cosine_similarity([user_embedding], [item_embedding])[0][0]\n",
    "            item_similarities.append((item_id, similarity))\n",
    "\n",
    "    return item_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(items, item_name_mapping, num_items, show_similarity=False):\n",
    "    f, ax = plt.subplots(1, num_items, figsize=(20,10))\n",
    "    if num_items == 1:\n",
    "        ax = [ax]\n",
    "    for i, item in enumerate(items):\n",
    "        item_id, similarity = item\n",
    "        print(f\"- Item {item_id}: {item_name_mapping[item_id]}\", end='')\n",
    "        if show_similarity:\n",
    "            print(f\" with similarity score: {similarity}\")\n",
    "        else:\n",
    "            print()\n",
    "        img_path = f\"../input/h-and-m-personalized-fashion-recommendations/images/0{str(item_id)[:2]}/0{int(item_id)}.jpg\"\n",
    "        try:\n",
    "            img = mpimg.imread(img_path)\n",
    "            ax[i].imshow(img)\n",
    "            ax[i].set_title(f'Item {item_id}')\n",
    "            ax[i].set_xticks([], [])\n",
    "            ax[i].set_yticks([], [])\n",
    "            ax[i].grid(False)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image for item {item_id} not found.\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(user_id, df, embeddings, item_name_mapping, num_items=5):\n",
    "    rated_items = get_rated_items(user_id, df)\n",
    "    \n",
    "    print(f\"User {user_id} has purchased:\")\n",
    "    show_images([(item_id, 0) for item_id in list(rated_items)[:5]], item_name_mapping, min(len(rated_items), 5))\n",
    "    \n",
    "    item_similarities = calculate_similarities(user_id, df, embeddings)\n",
    "\n",
    "    recommended_items = sorted(item_similarities, key=lambda x: x[1], reverse=True)[:num_items]\n",
    "\n",
    "    print(f\"\\nRecommended items for user {user_id}:\")\n",
    "    show_images(recommended_items, item_name_mapping, num_items, show_similarity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_items(45, GraphTravel_HM, embeddings, item_name_mapping, num_items=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
